{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IC/DN-D.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN4RKjxqRRw86Bkq0zzGR3G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Masoud-Karami/Abstracts/blob/main/IC_DN_D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMg-0RZW5zf0"
      },
      "source": [
        "#**[Demystifying Neural Language Models’ Insensitivity to Word-Order](https://arxiv.org/abs/2107.13955)**\n",
        "\n",
        "[code](https://github.com/chandar-lab/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRreRIhWSgxr"
      },
      "source": [
        ">## (In)Sensitivity of natural language models to word-order\n",
        "><ol>\n",
        "    <li><i><b>How to investigate:</b></i></li>\n",
        "    <ol type=\"a\">\n",
        "        <li><code>quantifying</code> perturbations and analyzing their effect on neural models’ performance on language understanding tasks in GLUE benchmark.</li>\n",
        "        <li>score the <code>local</code> and <code>global</code> ordering of tokens in the perturbed texts with the two following metrics:\n",
        "            <ul>\n",
        "                <li><b>local metric:</b> Direct Neighbour Displacement (DND)</li>\n",
        ">\n",
        ">> For every $c_j$, let $\\mathcal{N}^{x_i}(c_j, R)$ indicate the relative position of the right neighbor ($R$) of character $c_j$ with respect to the position of $c_j$ in string $x_i$. \n",
        "Then, DND is computed as a summation over an indicator variable that indicates when the neighbor to the right of $c_i$ has shifted to a different position in $x'_i$.\n",
        "    \\begin{equation}\n",
        "DND \\gets \\frac{1}{k-1}\\sum_{j=1}^{k-1} \\left(\\mathcal{1} \\left[ \\mathcal{N}^{x_i}(c_j, R) \\neq \\mathcal{N}^{x'_i}(c_j, R) \\right]\\right)\n",
        "    \\end{equation}\n",
        ">\n",
        "><li><b>global metric:</b> Index Displacement Count (IDC)</li>\n",
        ">                \n",
        ">> Let a string, $x_i = (c)_k^i$, be denoted by a sequence of characters $c_0, \\ldots, c_k$, where $k$ is the length of the string in characters and $p^{x_i}$ denote the positions of characters in $x_i$.  Let $\\eta(\\cdot)$ be a perturbation operation.\n",
        "                \\begin{equation}\n",
        "                x_i'\\gets \\eta\\left(x_i\\right),\n",
        "                \\end{equation}\n",
        "                where $x_i'$ denote the perturbed string with positions of the characters specified by $p^{x'_i}$.\n",
        "                \\begin{equation}\n",
        "                IDC \\gets \\frac{1}{k^2}\\sum_{j=1}^{k}\\left\\Vert p^{x'_i}\\left(j\\right) - p^{x_i}\\left(j\\right) \\right\\Vert_{1}\n",
        "                \\end{equation}\n",
        "            </ul>\n",
        "        </ol>\n",
        "        <li><i><b>Text structures:</b></i></li>\n",
        "        <ul type='none'> \n",
        "            <li><i><code>global:</code></i> which relates to the absolute position of characters to their immediate neighbors</li>\n",
        "            <li><i><code>local:</code></i> which relates to the relative position of characters to their immediate neighbors</li>\n",
        "        </ul>\n",
        "        <li><i><b>Achievements:</b></i></li>\n",
        "        <ol type='i'>\n",
        "            <li>perturbation functions found in prior literature affect only the global ordering while the local ordering remains relatively unperturbed.</li>\n",
        "            <li>pretrained and non-pretrained Transformers LSTMs, and Convolutional architectures require local ordering more so than the global ordering of tokens.</li>\n",
        "            <li> evaluating sentence comprehension mechanisms of human shows that specific orders of words is necessary for comprehending the text</li>\n",
        "        </ol>\n",
        "        <li><i><b>class of perturbation analysis</b></i></li>\n",
        "        <ol type=\"i\">\n",
        "            <li>deletion</li>\n",
        "            <li>paraphrase injection</li>\n",
        "            <li>perturbations at a finer granularity:</li>\n",
        "            <ul type=\"square\">\n",
        "                <li>word-level perturbed sentence</li>\n",
        "                <li>subword-level perturbed sentence</li>\n",
        "                <li> character-level perturbed sentence</li>\n",
        "            </ul>\n",
        "            On NLP, shuffleing n-grams (different values for n) to highlight the insensitivity of pretrained models shows that shuffling larger n-grams have a lesser effect than shuffling smaller n-grams\n",
        "        </ol>\n",
        "        <li><i><b>BERT models:</b></i></li>\n",
        "        <ol type='i'>\n",
        "            <li> have some syntactic capacity. \n",
        "            <li> represent information hierarchically and model linguistically relevant aspects in a hierarchical structure.</li>\n",
        "            <li>s' contextual embeddings outputs contain syntactic information that could be used in downstream tasks.</li>\n",
        "            <li> pretraining on syntax does not seem to improve downstream performance much.</li>\n",
        "            <li> can understand syntax but they often prefer not to use that information to solve tasks.</li>\n",
        "            <li> large language models are insensitive to minor perturbations highlighting the lack of syntactic knowledge used in syntax rich NLP tasks.</li>\n",
        "            <li> pretraining models on perturbed inputs still obtain reasonable results on downstream tasks, showing that models that have never been trained on well-formed syntax can obtain results that are close to their peers.</li>\n",
        "        </ol>\n",
        "        <li><i><b>Popular similarity metrics:</b></i></li>\n",
        "        <ul type=\"none\">\n",
        "        <li><code><b>BLEU</b> and <b>ROUGE</b>:</code>treat text as a sequence of words, from which a measure of overlap is computed.</li>\n",
        "        <li><code><b>Levenshtein</b> or <b>edit</b> distance:</code> measures the minimum amount of single character edits (insertions, deletions or substitutions) necessary to match two strings together.</li>\n",
        "        <li><code><b>Learned metrics:</b></code> which are often unaffected by minor perturbations in text which limits their usefulness in measuring perturbations.</li>\n",
        "        <ul>\n",
        "            <li><code><b>BERT-Score</b></code></li>\n",
        "            <li><code><b>BLEURT</b></code></li>\n",
        "            <li><code><b>POS mini-tree overlap score:</b></code> to computes the part-of-speech (PoS) tags neighborhood for every word and estimates an average overlap in the neighborhood for all the tokens before and after applying the perturbation.</li>\n",
        "        </ul>\n",
        "    </ul>\n",
        "    <li><code><b>B</b></code></li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1mqLIa3rPQW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}