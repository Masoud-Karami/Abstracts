{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IC/DN-D.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNqBpNw8FaxUMiejQ2VqMCO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Masoud-Karami/Abstracts/blob/main/IC_DN_D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMg-0RZW5zf0"
      },
      "source": [
        "#**[Demystifying Neural Language Models’ Insensitivity to Word-Order](https://arxiv.org/abs/2107.13955)**\n",
        "\n",
        "[code](https://github.com/chandar-lab/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRreRIhWSgxr"
      },
      "source": [
        ">## (In)Sensitivity of natural language models to word-order\n",
        "><ol>\n",
        "<li><i><b>How to investigate:</b></i></li>\n",
        "<ol type=\"a\">\n",
        "<li><code>quantifying</code> perturbations and analyzing their effect on neural models’ performance on language understanding tasks in GLUE benchmark.</li>\n",
        "<li>score the <code>local</code> and <code>global</code> ordering of tokens in the perturbed texts with the two following metrics:\n",
        "<ul>\n",
        "<li><b>local metric:</b> Direct Neighbour Displacement (DND)</li>\n",
        "<li><b>global metric:</b> Index Displacement Count (IDC)</li>\n",
        "</ul>\n",
        "</ol>\n",
        "<li><i><b>Text structures:</b></i></li>\n",
        "<ul type='none'> \n",
        "<li><i><code>global:</code></i> which relates to the absolute position of characters to their immediate neighbors</li>\n",
        "<li><i><code>local:</code></i> which relates to the relative position of characters to their immediate neighbors</li>\n",
        "</ul>\n",
        "<li><i><b>Achievements:</b></i></li>\n",
        "<ol type='i'>\n",
        "            <li>perturbation functions found in prior literature affect only the global ordering while the local ordering remains relatively unperturbed.</li>\n",
        "            <li>pretrained and non-pretrained Transformers LSTMs, and Convolutional architectures require local ordering more so than the global ordering of tokens.</li>\n",
        "            <li> evaluating sentence comprehension mechanisms of human shows that specific orders of words is necessary for comprehending the text</li>\n",
        "            </ol>\n",
        "            <li><i><b>class of perturbation analysis</b></i></li>\n",
        "            <ol type=\"i\">\n",
        "            <li>deletion</li>\n",
        "            <li>paraphrase injection</li>\n",
        "            <li>perturbations at a finer granularity:</li>\n",
        "            <ul type=\"square\">\n",
        "            <li>word-level perturbed sentence</li>\n",
        "            <li>subword-level perturbed sentence</li>\n",
        "            <li> character-level perturbed sentence</li>\n",
        "            </ul>\n",
        "            On NLP, shuffleing n-grams (different values for n) to highlight the insensitivity of pretrained models shows that shuffling larger n-grams have a lesser effect than shuffling smaller n-grams\n",
        "            </ol>\n",
        "            <li><i><b>BERT models:</b></i></li>\n",
        "            <ol type='i'>\n",
        "            <li> have some syntactic capacity. \n",
        "            <li> represent information hierarchically and model linguistically relevant aspects in a hierarchical structure.</li>\n",
        "            <li>s' contextual embeddings outputs contain syntactic information that could be used in downstream tasks.</li>\n",
        "            <li> pretraining on syntax does not seem to improve downstream performance much.</li>\n",
        "            <li> can understand syntax but they often prefer not to use that information to solve tasks.</li>\n",
        "            <li> large language models are insensitive to minor perturbations highlighting the lack of syntactic knowledge used in syntax rich NLP tasks.</li>\n",
        "            <li> pretraining models on perturbed inputs still obtain reasonable results on downstream tasks, showing that models that have never been trained on well-formed syntax can obtain results that are close to their peers.</li>\n",
        "            </ol>\n",
        "            <li><i><b>Popular similarity metrics:</b></i></li>\n",
        "            <ul type=\"none\">\n",
        "            <li><code><b>BLEU</b> and <b>ROUGE</b>:</code>treat text as a sequence of words, from which a measure of overlap is computed.</li>\n",
        "            <li><code><b>Levenshtein</b> or <b>edit</b> distance:</code> measures the minimum amount of single character edits (insertions, deletions or substitutions) necessary to match two strings together.</li>\n",
        "            <li><code><b>Learned metrics:</b></code> which are often unaffected by minor perturbations in text which limits their usefulness in measuring perturbations.</li>\n",
        "            <ul>\n",
        "            <li><code><b>BERT-Score</b></code></li>\n",
        "            <li><code><b>BLEURT</b></code></li>\n",
        "            <li><code><b>POS mini-tree overlap score:</b></code> to computes the part-of-speech (PoS) tags neighborhood for every word and estimates an average overlap in the neighborhood for all the tokens before and after applying the perturbation.</li>\n",
        "            </ul>\n",
        "            </ul>\n",
        "            <li><code><b>B</b></code></li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1mqLIa3rPQW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}