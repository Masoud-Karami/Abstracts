{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Masoud-Karami/Abstracts/blob/main/Word_Salad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMg-0RZW5zf0"
   },
   "source": [
    "#<a href=https://arxiv.org/abs/2101.03453><b>BERT & Family Eat Word Salad:</b> Experiments with Text Understanding</a>\n",
    "<a href=https://github.com/utahnlp/word-salad>code</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRreRIhWSgxr"
   },
   "source": [
    "#**The goal**:\n",
    "<ul>\n",
    "<li>study the response of large neural models to <code><i>destructive transformations</i></code>: perturbations of inputs that render them meaningless.</li>\n",
    "</ul>\n",
    "\n",
    "# **To be argued**:\n",
    "A reliable model should not be insensitive to such a drastic change in word order.\n",
    "\n",
    "#**How**:\n",
    "<ol type=\"i\">\n",
    "<li>defining simple heuristics to construct incoherent inputs that should confuse any model that claims to understand natural language.</li>\n",
    "<li>characterizing the models' response using two metrics:\n",
    "<ul type=\"square\">\n",
    "<li>its ability to predict valid labels for invalid input</li>\n",
    "<li>its confidence on these predictions</li>\n",
    "</ul></li>\n",
    "<li>evaluating strategies to mitigate these weaknesses using regularization that makes models less confident in their predictions, or by allowing models to reject inputs.\n",
    "</li>\n",
    "</ol>\n",
    "___\n",
    "<ol>\n",
    "<li><i><b>Destructive Transformation:</b></i>\n",
    "\n",
    ">Consider a task with input $x \\in X$ and an oracle function $f$ that maps inputs to labels $y \\in Y$. A destructive transformation $\\pi: X \\to X$ is a function that operates on $x$ to produce transformed inputs $x^\\prime = \\pi(x)$ such that $f(x^\\prime)$ is undefined. That is, none of the labels (i.e., the set $Y$) can apply to $x^\\prime$.\n",
    "\n",
    "Different classes of transformations:\n",
    "<ul type=\"none\">\n",
    "<li><code>Lexical Overlap-based Transformations:</code> which preserve the bag-of-words representation of the original input but change the word order\n",
    "<ul type=\"I\">\n",
    "<li><b>Sort</b></li>\n",
    "<li><b>Shuffle</b></li>\n",
    "<li><b>Reverse</b></li>\n",
    "<li><b>CopySort:</b> Copy one of the input texts and then sort it to create the second text.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><code>Gradient-based Transformations:</code>\n",
    "\n",
    "scoring input tokens in proportion to their relative contribution to the output then studying the impact of removing, repeating, and replacing scored tokens.\n",
    "\n",
    ">Given a trained neural model $\\mathcal{M}$, and the task loss function $\\mathcal{L}$, the change in the loss for the $i^{th}$ input token is approximated by the dot product of its token embedding $\\mathbf{t}_i$ and the gradient of the loss propagated back to the input layer $\\nabla_{\\mathbf{t}_i, \\mathcal{M}}\\mathcal{L}$. That is, the $i^{th}$ token is scored by  $\\mathbf{t}_i^\\intercal\\nabla_{\\mathbf{t}_i,\\mathcal{M}}\\mathcal{L}$. \n",
    "\n",
    "A higher score denotes a more important token.\n",
    "<li><code>Statistical transformation: PBSMT</code> \n",
    "\n",
    "Phrasebased statistical machine translation (PBSMT) system to generate examples that use phrasal co-occurrence statistics.\n",
    "\n",
    "</li>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><i><b>Achievements:</b></i>\n",
    "<ol type=\"a\">\n",
    "        <li>the labels predicted by state-of-the-art models for destructively transformed inputs bear high agreement with the original ones</li>\n",
    "        <li>models trained on meaningless examples perform comparably to the original model on unperturbed examples, despite never having encountered any well-formed training examples.</li>\n",
    "        <li>models trained on meaningless sentences constructed by permuting the word order perform almost as well as the state-of-the-art models.</li>\n",
    "        <li>models struggle even with the form of language by demonstrating that they force meaning onto token sequences devoid of any, i.e., they are not using the right kind of information to arrive at their predictions</li>\n",
    "        <li></li>\n",
    "        <li></li>\n",
    "        <li></li>\n",
    "        <li></li>\n",
    "        <li></li>\n",
    "        </ol>\n",
    "</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP8Fly64tbnOWy62GOMAuyd",
   "include_colab_link": true,
   "name": "Word-Salad.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
