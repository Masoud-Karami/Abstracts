{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Masoud-Karami/Abstracts/blob/main/Word_Salad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMg-0RZW5zf0"
   },
   "source": [
    "#<a href=https://arxiv.org/abs/2101.03453><b>BERT & Family Eat Word Salad:</b> Experiments with Text Understanding</a>\n",
    "\n",
    "<a href=https://github.com/utahnlp/word-salad>code</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRreRIhWSgxr"
   },
   "source": [
    "#**The goal**:\n",
    "<ul>\n",
    "    <li>study the response of large neural models to <code><i>destructive transformations</i></code>: perturbations of inputs that render them meaningless.\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "# **To be argued**:\n",
    "A <code>reliable model</code> <font size=\"-2\">(which is a model that should know what it does not know,\n",
    "not fail silently, and be uncertain on examples that are uninformative about the label.)</font> should not be insensitive to such a drastic change in word order.\n",
    "\n",
    "#**How**:\n",
    "<ol type=\"i\">\n",
    "    <li>defining simple heuristics to construct incoherent inputs that should confuse any model that claims to understand natural language.</li>\n",
    "    <li>characterizing the models' response using two metrics:\n",
    "        <ul type=\"square\">\n",
    "            <li>its ability to predict valid labels for invalid input</li>\n",
    "            <li>its confidence on these predictions</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>evaluating strategies to mitigate these weaknesses using regularization that makes models less confident in their predictions, or by allowing models to reject inputs.</li>\n",
    "</ol>\n",
    "___\n",
    "<ol>\n",
    "    <li><i><b>Destructive Transformation:</b></i>\n",
    "\n",
    ">Consider a task with input $x \\in X$ and an oracle function $f$ that maps inputs to labels $y \\in Y$. A destructive transformation $\\pi: X \\to X$ is a function that operates on $x$ to produce transformed inputs $x^\\prime = \\pi(x)$ such that $f(x^\\prime)$ is undefined. That is, none of the labels (i.e., the set $Y$) can apply to $x^\\prime$.\n",
    "\n",
    "Different classes of transformations:\n",
    "        <ul type=\"none\">\n",
    "            <li><code>Lexical Overlap-based Transformations:</code> which preserve the bag-of-words representation of the original input but change the word order\n",
    "                <ul type=\"I\">\n",
    "                    <li><b>Sort</b></li>\n",
    "                    <li><b>Shuffle</b></li>\n",
    "                    <li><b>Reverse</b></li>\n",
    "                    <li><b>CopySort:</b> Copy one of the input texts and then sort it to create the second text\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li><code>Gradient-based Transformations:</code>\n",
    "            scoring input tokens in proportion to their relative contribution to the output then studying the impact of removing, repeating, and replacing scored tokens.\n",
    "           \n",
    ">Given a trained neural model $\\mathcal{M}$, and the task loss function $\\mathcal{L}$, the change in the loss for the $i^{th}$ input token is approximated by the dot product of its token embedding $\\mathbf{t}_i$ and the gradient of the loss propagated back to the input layer $\\nabla_{\\mathbf{t}_i, \\mathcal{M}}\\mathcal{L}$. That is, the $i^{th}$ token is scored by  $\\mathbf{t}_i^\\intercal\\nabla_{\\mathbf{t}_i,\\mathcal{M}}\\mathcal{L}$.\n",
    "                \n",
    "A higher score denotes a more important token.\n",
    "                </li>\n",
    "                    <li><code>Statistical transformation: PBSMT</code> \n",
    "            Phrase-based statistical machine translation (PBSMT) system to generate examples that use phrasal co-occurrence statistics.\n",
    "                    </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><i><b>A <code>reliable model</code> should exhibit the following:</b></i>\n",
    "        <ol type=\"i\">\n",
    "            <li>the agreement between original predictions and predictions on their transformed invalid variants should be random,</li>\n",
    "            <li>predictions for invalid examples should be uncertain</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><i><b>Two metrics are necessary to accomplish step $2$:</b></i>\n",
    "        <ul type=\"none\">\n",
    "            <li><code>Agreement</code> which is the $%$ of examples whose prediction remains\n",
    "same after applying a destructive transformation. High agreement scores show that models retain their original predictions even when labelbearing information is removed from examples.\n",
    "$$\\text{closer to random} \\implies \\text{better handle invalid examples}$$</li>\n",
    "            <li><code>Confidence</code> <font size=\"-2\">(and entropy of output distributions reveal the same insights)</font> which is defined as the average probability of the predicted label. We want this number to be closer to $\\frac{1}{N}$, where $N$ is the number of classes.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><i><b>Calibrated <code>BERT</code> test on invalid examples:</b></i>\n",
    "        by training confidence calibrated classifiers using following standard methods:\n",
    "        <ol type=I>\n",
    "            <li><a href=https://arxiv.org/abs/1906.02629>$\\text{label smooting}$</a> (<a href=https://youtu.be/wmUiOAra_-M>a quick intro</a>) with <a href=https://amaarora.github.io/2020/06/29/FocalLoss.html> $\\text{Focal loss}$</a></li>\n",
    "            <li><a href=https://arxiv.org/abs/1910.12656>$\\text{Temperatur scaling}$</a> (<a href=https://geoffpleiss.com/nn_calibration>intro </a>)</li>\n",
    "            <li><a href=https://arxiv.org/abs/1706.04599>$\\text{Expected calibration error (ECE)}$</a> Better calibrated models have lower ECE</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><i><b>Small vs. large perturbations:</b></i>\n",
    "        <ul>\n",
    "            <li>Robustness of the model to small input perturbations is <b>desirable</b> (<font size=\"-2\">modelâ€™s prediction should not change for small perturbations in the input.</font>)</li>\n",
    "            <li>However, excessive invariance to large input perturbations is <b>undesirable</b>.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><i><b>Achievements:</b></i>\n",
    "    <ol type=\"a\">\n",
    "        <li>the labels predicted by state-of-the-art models for destructively transformed inputs bear high agreement with the original ones</li>\n",
    "        <li>models trained on meaningless examples perform comparably to the original model on unperturbed examples, despite never having encountered any well-formed training examples.</li>\n",
    "        <li>models trained on meaningless sentences constructed by permuting the word order perform almost as well as the state-of-the-art models.</li>\n",
    "        <li>models struggle even with the form of language by demonstrating that they force meaning onto token sequences devoid of any, i.e., they are not using the right kind of information to arrive at their predictions</li>\n",
    "        <li> The transformations render sentences meaningless to humans, but the model knows the label.</li>\n",
    "        <li>It is possible that, rather than understanding text, they merely learn spurious correlations in the training data. That is, models use the wrong information to arrive at the right answer.</li>\n",
    "        <li>Not only do models retain a large fraction of their predictions, they do so with high confidence</li>\n",
    "        <li>The reason why this undesirable model behavior occur in all models, irrespective of the pretraining tasks (and is even seen in models with a recurrent inductive bias) is that because these <a href=https://arxiv.org/abs/1803.02324> large models learn spurious correlations present in the training datasets</a>.\n",
    "            <ul>\n",
    "                <li>To substantiates this claim, train the model by flipping the (<mark>training set</mark>=<b style=\"background-color:lightgreen;\">valid</b>, validation set=<b style=\"background-color:red;\">invalid</b>) to (<mark>training set</mark>=<b style=\"background-color:red;\">invalid</b>, validation set=<b style=\"background-color:lightgreen;\">valid</b>)</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li></li>\n",
    "    </ol>\n",
    "</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP8Fly64tbnOWy62GOMAuyd",
   "include_colab_link": true,
   "name": "Word-Salad.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
