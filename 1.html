>## (In)Sensitivity of natural language models to word-order
><ol>
    <li><i><b>How to investigate:</b></i></li>
    <ol type="a">
        <li><code>quantifying</code> perturbations and analyzing their effect on neural modelsâ€™ performance on language understanding tasks in GLUE benchmark.</li>
        <li>score the <code>local</code> and <code>global</code> ordering of tokens in the perturbed texts with the two following metrics:
            <ul>
                <li><b>local metric:</b> Direct Neighbour Displacement (DND)</li>
                For every $c_j$, let $\mathcal{N}^{x_i}(c_j, R)$ indicate the relative position of the right neighbor ($R$) of character $c_j$ with respect to the position of $c_j$ in string $x_i$. Then, DND is computed as a summation over an indicator variable that indicates when the neighbor to the right of $c_i$ has shifted to a different position in $x'_i$.
                \begin{equation}
                DND \gets \frac{1}{k-1}\sum_{j=1}^{k-1} \left(1 \left[ \mathcal{N}^{x_i}(c_j, R) \neq\mathcal{N}^{x'_i}(c_j, R) \right]\right)
                \end{equation}
                <li><b>global metric:</b> Index Displacement Count (IDC)</li>
                Let a string, $x_i = (c)_k^i$, be denoted by a sequence of characters $c_0, \ldots, c_k$, where $k$ is the length of the string in characters and $p^{x_i}$ denote the positions of characters in $x_i$.  Let $\eta(\cdot)$ be a perturbation operation.
                \begin{equation}
                x_i'\gets \eta\left(x_i\right),
                \end{equation}
                where $x_i'$ denote the perturbed string with positions of the characters specified by $p^{x'_i}$.
                \begin{equation}
                IDC \gets \frac{1}{k^2}\sum_{j=1}^{k}\left\Vert p^{x'_i}\left(j\right) - p^{x_i}\left(j\right) \right\Vert_{1}
                \end{equation}
            </ul>
        </ol>
        <li><i><b>Text structures:</b></i></li
            <ul type='none'>
                <li><i><code>global:</code></i> which relates to the absolute position of characters to their immediate neighbors</li>
                <li><i><code>local:</code></i> which relates to the relative position of characters to their immediate neighbors</li>
            </ul>
            <li><i><b>Achievements:</b></i></li>
            <ol type='i'>
                <li>perturbation functions found in prior literature affect only the global ordering while the local ordering remains relatively unperturbed.</li>
                <li>pretrained and non-pretrained Transformers LSTMs, and Convolutional architectures require local ordering more so than the global ordering of tokens.</li>
                <li> evaluating sentence comprehension mechanisms of human shows that specific orders of words is necessary for comprehending the text</li>
            </ol>
            <li><i><b>class of perturbation analysis</b></i></li>
            <ol type="i">
                <li>deletion</li>
                <li>paraphrase injection</li>
                <li>perturbations at a finer granularity:</li>
                <ul type="square">
                    <li>word-level perturbed sentence</li>
                    <li>subword-level perturbed sentence</li>
                    <li> character-level perturbed sentence</li>
                </ul>
                On NLP, shuffleing n-grams (different values for n) to highlight the insensitivity of pretrained models shows that shuffling larger n-grams have a lesser effect than shuffling smaller n-grams
            </ol>
            <li><i><b>BERT models:</b></i></li>
            <ol type='i'>
                <li> have some syntactic capacity.
                <li> represent information hierarchically and model linguistically relevant aspects in a hierarchical structure.</li>
                <li>s' contextual embeddings outputs contain syntactic information that could be used in downstream tasks.</li>
                <li> pretraining on syntax does not seem to improve downstream performance much.</li>
                <li> can understand syntax but they often prefer not to use that information to solve tasks.</li>
                <li> large language models are insensitive to minor perturbations highlighting the lack of syntactic knowledge used in syntax rich NLP tasks.</li>
                <li> pretraining models on perturbed inputs still obtain reasonable results on downstream tasks, showing that models that have never been trained on well-formed syntax can obtain results that are close to their peers.</li>
            </ol>
            <li><i><b>Popular similarity metrics:</b></i></li>
            <ul type="none">
                <li><code><b>BLEU</b> and <b>ROUGE</b>:</code>treat text as a sequence of words, from which a measure of overlap is computed.</li>
                <li><code><b>Levenshtein</b> or <b>edit</b> distance:</code> measures the minimum amount of single character edits (insertions, deletions or substitutions) necessary to match two strings together.</li>
                <li><code><b>Learned metrics:</b></code> which are often unaffected by minor perturbations in text which limits their usefulness in measuring perturbations.</li>
                <ul>
                    <li><code><b>BERT-Score</b></code></li>
                    <li><code><b>BLEURT</b></code></li>
                    <li><code><b>POS mini-tree overlap score:</b></code> to computes the part-of-speech (PoS) tags neighborhood for every word and estimates an average overlap in the neighborhood for all the tokens before and after applying the perturbation.</li>
                </ul>
            </ul>
            <li><code><b>B</b></code></li>
        </ol>
