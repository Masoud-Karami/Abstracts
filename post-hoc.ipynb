{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "befc84c6",
   "metadata": {},
   "source": [
    "<h1> <a href=https://arxiv.org/abs/2108.04840><b>Post-hoc Interpretability for Neural NLP:</b> A Survey</a></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0297f16a",
   "metadata": {},
   "source": [
    "<h2>This survey:</h2>\n",
    "<ul>\n",
    "    <li>provides a categorization of how recent interpretability methods communicate explanations and discusses the methods in depth</li>\n",
    "    <li>focuses on post-hoc methods, which provide explanations after a model is learned and generally model-agnostic</li>\n",
    "    <li>discusses how these post-hoc methods are evaluated since the concern is whether these interpretabilities accurately reflect the model.</li>\n",
    "    <li></li>\n",
    "    <li></li>\n",
    "</ul>\n",
    "---\n",
    "<h2>Motivation:</h2>\n",
    "<ul>\n",
    "    <li>Interpretability:\n",
    "        <ol>\n",
    "            <li>Due to their immense complexity motivated by general correlation between their size and test performance, <b>balck-box</b> models exhibit unwanted biases and similar ethical issues,</li>\n",
    "            <li>these issues stem from an <i style=\"background-color:orange;\">``incompleteness in the problem formalization``</i></li>\n",
    "            <li> these issues can be partially prevented (but not for all failure modes) with $robustness$ and $fairness$ metrics</li>\n",
    "            <li><b>The goal of interpretability</b> is therefore providing <i style=\"background-color:lime;\">$\\text{quality assessment}$</i> through <i style=\"background-color:lime;\">$\\text{model explanations}$</i> to to facilitate the <i style=\"background-color:lime;\">$\\text{accountability process}$</i></li>\n",
    "            <li>definition (<a href=https://arxiv.org/abs/1702.08608>Doshi-Velez and Kim (2017)</a>): <i>the ability to explain or to present in <b style=\"background-color:yellow\">understandable</b> terms to a human</i></li>\n",
    "            <li>We consider <b style=\"background-color:yellow\">understandable</b> as an interdisciplinary concept</li>\n",
    "            <li>definition (<a href=https://arxiv.org/abs/1706.07269>Miller (2019)</a>): effective explanations must be selective in the sense one must select “one or two causes from a sometimes infinite number of causes”.  Such observation necessitates organizing interpretability methods by <i style=\"background-color:lightCoral\">how</i> and <i style=\"background-color:lightCoral\">what</i> they selectively <b>communicate</b>.</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li></li>\n",
    "</ul>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e064eed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a37542",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
