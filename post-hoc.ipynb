{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "befc84c6",
   "metadata": {},
   "source": [
    "<h1> <a href=https://arxiv.org/abs/2108.04840><b>Post-hoc Interpretability for Neural NLP:</b> A Survey</a></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0297f16a",
   "metadata": {},
   "source": [
    "<h2>This survey:</h2>\n",
    "<ul>\n",
    "    <li>provides a categorization of how recent interpretability methods communicate explanations and discusses the methods in depth</li>\n",
    "    <li>focuses on post-hoc methods, which provide explanations after a model is learned and generally model-agnostic</li>\n",
    "    <li>discusses how these post-hoc methods are evaluated since the concern is whether these interpretabilities accurately reflect the model.</li>\n",
    "    <li>\n",
    "        <table>\n",
    "            <tr>\n",
    "                <th style=\"height:30px;width:50px;text-align: center;background-color: #96D4D4;\">$\\text{post-hoc}$</th> \n",
    "                <th style=\"height:30px;width:50px;text-align: center;background-color: #96D4D4;\"> $\\text{intrinsic}$</th>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"height:50px;width:300px;text-align: left;\">methods that provide their explanation after a model is trained and are often model-agnostic</td>\n",
    "                <td style=\"height:50px;width:300px;text-align: left;\">the model architecture itself helps to provide the explanation $\\implies$ interpretable by design</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"height:50px;width:300px;text-align: left;\">valuable in legal proceedings, where models may need to be explained retroactively,\n",
    "                </td>\n",
    "                <td style=\"height:50px;width:300px;text-align: left;\">more responsible to use in high-stakes decision processes,\n",
    "                </td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"height:50px;width:300px;text-align: left;\">it is guaranteed that they will not affect model performance.\n",
    "                </td>\n",
    "                <td style=\"height:50px;width:300px;text-align: left;\">\n",
    "                </td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </li>\n",
    "    <li>presents organizational interpretability methods by how and what they selectively communicate. For example, <i>input feature </i>explanations communicate what tokens are most relevant for a prediction\n",
    "    </li>\n",
    "    <li>Communication strategies' abstraction levels:\n",
    "        <table>\n",
    "            <tr>\n",
    "                <th style=\"height:30px;width:50px;text-align: center;background-color: #96D4D4;\">$\\text{less abstract}$</th>\n",
    "                <th style=\"height:30px;width:50px;text-align: center;background-color: #96D4D4;\">$\\text{more abstract}$</th>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"height:30px;width:300px;text-align: center;\">local explanations</td>\n",
    "                <td style=\"height:30px;width:300px;text-align: center;\">global explanations</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"height:100px;width:300px;text-align: left;\">\n",
    "                    <ul>\n",
    "                        <li>Input Features</li>\n",
    "                        <li></li>\n",
    "                        <li></li>\n",
    "                    </ul>\n",
    "                </td>\n",
    "                <td style=\"height:100px;width:300px;text-align: left;\">global explanations</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"height:50px;width:300px;text-align: left;\">e.g., <b>input features explanation</b>\n",
    "which highlights the input tokens that are most responsible for a prediction refer to <code>specific tokens</code>, therefore, its ability to provide abstract explanations is limited.</td>\n",
    "                <td style=\"height:50px;width:300px;text-align: left;\">e.g., <b>natural language</b> category which explains a prediction using <code>a sentence</code> and can therefore use abstract concepts in its explanation.</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"height:50px;width:300px;text-align: left;\">not easier to understand, but the trade-off is that they may reflect the model’s behavior <b>more</b></td>\n",
    "                <td style=\"height:50px;width:300px;text-align: left;\">easier to understand, but the trade-off is that they may reflect the model’s behavior <b>less</b></td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </li>\n",
    "</ul>\n",
    "</ul>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f1b17d",
   "metadata": {},
   "source": [
    "<h2>Accountability:</h2>\n",
    "<ul>\n",
    "    <li><b>definition:</b>\n",
    "        <ul>\n",
    "            <li> “the quality or state of being accountable, i.e., subject to giving an account and capable of being explained”. Accordingly, accountability is mainly related to auditing algorithms and verifying they perform as expected.</li>\n",
    "            <li>The ability to inspect a model in post hoc, and make it available for human or algorithmic inspection.</li>\n",
    "            <li>“Accountable” means “expected or required to account for one’s actions; answerable.” “Accountability” therefore means the ability, inclination or suitability to make someone answerable for his or her actions. According to the OECD, accountability means the ability to place the onus on the\n",
    "appropriate organizations or individuals for the proper functioning of AI systems</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "<h2>Interpretability:</h2>\n",
    "<ol>\n",
    "    <li><b>the goal:</b> is to communicate the model to a human (often qualitatively)</li>\n",
    "    <li>Due to their immense complexity motivated by general correlation between their size and test performance, <b>balck-box</b> models exhibit unwanted biases and similar ethical issues,</li>\n",
    "    <li>these issues stem from an <i style=\"background-color:orange;\">``incompleteness in the problem formalization``.</i>, meaning if the model was contrained and optimized to prevent all possible ethical issues, interpretability would be much less relevant</li>\n",
    "    <li> these issues can be partially prevented (but not for all failure modes) with $robustness$ and $fairness$ metrics</li>\n",
    "    <li><b>The goal of interpretability</b> is therefore providing <i style=\"background-color:lime;\">$\\text{quality assessment}$</i> through <i style=\"background-color:lime;\">$\\text{model explanations}$</i> to to facilitate the <i style=\"background-color:lime;\">$\\text{accountability process}$</i></li>\n",
    "    <li>definition (<a href=https://arxiv.org/abs/1702.08608>Doshi-Velez and Kim (2017)</a>): <i>the ability to explain or to present in <b style=\"background-color:yellow;\">understandable</b> terms to a human</i></li>\n",
    "    <li>We consider <b style=\"background-color:yellow;\">understandable</b> as an interdisciplinary concept</li>\n",
    "    <li>definition (<a href=https://arxiv.org/abs/1706.07269>Miller (2019)</a>): effective explanations must be selective in the sense one must select “one or two causes from a sometimes infinite number of causes”.  Such observation necessitates organizing interpretability methods by <i style=\"background-color:red;\">how</i> and <i style=\"background-color:red;\">what</i> they selectively <b>communicate</b>.</li>\n",
    "    <li><b>Adversarial exmaples</b> (<i>if not produced form and injected to an existing example</i>) \n",
    "        have nothing to do with the actual input $\\implies$ does not explain the model’s support boundary \n",
    "        and is therefore <s>unrelated to interpretability</s> but more related to <b>robustness</b>\n",
    "    </li>\n",
    "</ol>\n",
    "<h2>Communication:</h2>\n",
    "        <ul>\n",
    "            <li>communication methods in higher abstraction level are easier to understrand but they may reflect the model’s behavior less</li>\n",
    "            <li>a trade-off between <b>abstraction</b> and <b>model behaviour</b> is necessary</li>\n",
    "        </ul>\n",
    "<h2>Motivation for interpretability:</h2>\n",
    "<ol type=\"A\">\n",
    "    <li><b>incompleteness in the problem formalization</b></li>\n",
    "    <li><b>safety</b>\n",
    "        ensuring the model performs within expectations in deployment\n",
    "        <ul>\n",
    "            <li> it is nearly impossible to truly test the model,</li>\n",
    "            <li>adversarial examples and counterfactuals are useful, as they evaluate the model on data outside the test distribution.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>ethics</b>\n",
    "        ensuring that the model’s behavior is aligned with common ethical and moral values\n",
    "        <ul>\n",
    "            <li>should be judged qualitatively by humans,</li>\n",
    "            <li>to some extend involve qualitative assessment,</li>\n",
    "            <li>in some cases it may be possible to measure and satisfy this ethical concern via fairness metrics and debiasing techniques</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>accountability</b>\n",
    "    relates to explaining the model when it does fail in production\n",
    "    </li>\n",
    "    <li><b>scientific understanding</b>\n",
    "    addresses a need by researchers and scientists, which is to enerate hypotheses and knowledge\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126ec7ef",
   "metadata": {},
   "source": [
    "<h2>Measuring the interpretability:</h2>\n",
    "<ol type=\"i\">\n",
    "    <li><b>Application-grounded</b>\n",
    "        <ol>\n",
    "            <li>directly related to the model</li>\n",
    "            <li>the interpretability method is evaluated in the environment it will be deployed</li>\n",
    "            <li>should include the baseline where the explanations are provided by humans</li>\n",
    "            <li>application-specific and long-term nature of this approach $\\implies$  rarely done in NLP</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><b>Human-grounded:</b>\n",
    "        <ol>\n",
    "            <li>also known as <b>plausibility</b>, <b>simulatability</b>, and\n",
    "<b>comprehensibility</b></li>\n",
    "            <li>checks if the explanations are useful to lay humans instead of domain experts</li>\n",
    "            <li>the task is often simpler and can be evaluated immediately</li>\n",
    "            <li>often involves interdisciplinary knowledge from the human-computer interaction (HCI) and social science fields</li>\n",
    "            <li>can not reflect the model perfectly, because humans require explanations to be selective, meaning the explanation should select “one or two causes from a sometimes infinite number of causes</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><b>Functionally-grounded:</b>\n",
    "        <ol>\n",
    "            <li>also known as <b>faithfulness</b> and have also been referred to as <b>fidelity</b></li>\n",
    "            <li>checks how well the explanation reflects the model (but not the input for example)</li>\n",
    "            <li>checks if the explanations are useful to lay humans instead of domain experts</li>\n",
    "            <li>the task is often simpler and can be evaluated immediately</li>\n",
    "            <li>often involves interdisciplinary knowledge from the human-computer interaction (HCI) and social science fields</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><b>common evaluation strategies</b>\n",
    "        <ul>\n",
    "            <li>Comparing with an intrinsically interpretable model, such as <code>logistic regression</code></li>\n",
    "            <li>Comparing with other post-hoc methods</li>\n",
    "            <li>Proposing axiomatic desirables</li>\n",
    "            <li>Benchmarking against random explanations</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daca61a7",
   "metadata": {},
   "source": [
    "<h1>Interpretability methods</h1>\n",
    "<h2>Input Features:</h2>\n",
    "<ul>\n",
    "    <li>determine how important an input feature \n",
    "        (<i>always known and often meaningful to humans</i>), e.g. a token, is for a given prediction. \n",
    "        input $\\mathbf{x}$,\n",
    "        \\begin{equation}\n",
    "        \\mathbf{E}(\\mathbf{x}, c): \n",
    "        \\mathrm{I}^\\mathbf{d} \\rightarrow \\mathbb{R}^\\mathbf{d} \n",
    "        \\end{equation}\n",
    "        where$\\mathrm{I}$ is the input domain and $\\mathbf{d}$ is the input dimensionality.\n",
    "        <ul>\n",
    "            <li>when the output is a score of importance the explanation is called \n",
    "                an importance measure\n",
    "            </li>\n",
    "            <li>can only explain one class at a time\n",
    "            </li>\n",
    "        </ul>\n",
    "    <li><b>Gradient:</b>\n",
    "        \\begin{equation}\n",
    "        {\\mathbf{E}}_{gradient}(\\mathbf{x}, c) = \n",
    "        \\nabla_\\mathbf{x} f(\\mathbf{x})_c\n",
    "        \\end{equation}\n",
    "        where $f(\\mathbf{x})$ is the model logits.\n",
    "        <ul>\n",
    "            <li><a href=https://arxiv.org/pdf/1810.03292.pdf>for a linear model</a> \n",
    "                $f(\\mathbf{x}) = \\mathbf{x}\\mathbf{W}$, the explanation would be $\\mathbf{W}^\\top$ \n",
    "                which is clearly a valid explanation\n",
    "            </li>\n",
    "            <li>this does not guarantee functionally-groundedness \n",
    "                <a href=https://aclanthology.org/N16-1082>for non-linear models</a> \n",
    "                we approximate $f(\\mathbf{x})$ with a linear function by computing the \n",
    "                first-order Taylor expansion\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>Integrated Gradient (IG)</b>:\n",
    "        <ul>\n",
    "            <li>$Sensitivity$ means, if there exists a combination of $\\mathbf{x}$ and baseline \n",
    "                $\\mathbf{b}$ (often an empty sequence), where the output of $f(\\mathbf{x})$ and \n",
    "                $f(\\mathbf{b})$ are different, then the feature that changed should get a non-zero attribution. \n",
    "                This desirable is not satisfied for the gradient method, \n",
    "                for example due to the truncation  in $ReLU(\\cdot)$.</li>\n",
    "            <li>$Completeness$ means, the sum of importance scores assigned to each token should \n",
    "                equal the model output relative to the baseline $\\mathbf{b}$.\n",
    "                \\begin{equation}\n",
    "                \\mathbf{E}_{integrated-gradient}(\\mathbf{x}, c) = \n",
    "                (\\mathbf{x} - \\mathbf{b}) \\odot \\frac{1}{k} \\sum_{i=1}^{k} \n",
    "                \\nabla_{\\tilde{\\mathbf{x}}_i} f(\\tilde{\\mathbf{x}}_i)_c, \\quad \\tilde{\\mathbf{x}}_i = \n",
    "                \\mathbf{b} + \\frac{i}{k}(\\mathbf{x} - \\mathbf{b}) \n",
    "                \\end{equation}\n",
    "            </li>\n",
    "            <li>criticism in CV community for ------------------------------------> <s>not being functionally-grounded</s>. \n",
    "                One reason is that it multiples by the input, a signal that is not directly related to the model\n",
    "            </li>\n",
    "        </ul>\n",
    "    <li><b>LIME:</b>\n",
    "        <ul>\n",
    "            <li><b> --------------------------------------------------------> functionally-grounded and human-grounded</b></li>\n",
    "            <li>Instead of relying on gradient, it samples nearby observations $\\tilde{\\mathbf{x}}$ and \n",
    "                uses the model estimate $p(c|\\tilde{\\mathbf{x}})$ to fit a logistic regression. \n",
    "                The parameters $\\mathbf{w}$ of the logistic regression then represents the \n",
    "                importance measure, since larger parameters would mean a \n",
    "                greater effect on the output.\n",
    "                \\begin{equation}\n",
    "                {\\mathbf{E}}_{LIME}(\\mathbf{x}, c) =  argmin_{\\mathbf{w}} \n",
    "                \\frac{1}{k} \\sum_{i=1}^k \\left(p(c|\\tilde{\\mathbf{x}}_i) \\log(q(\\tilde{\\mathbf{x}}_i)) + \n",
    "                (1-p(c|\\tilde{\\mathbf{x}}_i)) \\log(1-q(\\tilde{\\mathbf{x}}_i)\\right) + \n",
    "                \\lambda \\|\\mathbf{w}\\|_1 \\\\ \\text{ where } q(\\tilde{\\mathbf{x}}) = \n",
    "                \\sigma(\\mathbf{w} \\tilde{\\mathbf{x}})\n",
    "                \\end{equation}\n",
    "            </li>\n",
    "            <li>how to sample $\\tilde{\\mathbf{x}}$ representing the nearby observations:\n",
    "                <ol>\n",
    "                    <li>use a Bag-Of-Words (BoW) representation with a cosine distance \n",
    "                        (distance metrics may not effectively match the model's internal space)</li>\n",
    "                    <li>sample $\\tilde{\\mathbf{x}}$ by masking words of $\\mathbf{x}$ \n",
    "                        (requires a model that supports such masking)</li>\n",
    "                </ol>\n",
    "            </li>\n",
    "            <li><b>advantages:</b> \n",
    "                <ol>\n",
    "                    <li>only depends on black-box information and the dataset, therefore \n",
    "                        no gradient calculations are required</li>\n",
    "                    <li>it uses a <code>LASSO logistic regression</code>, which is a \n",
    "                                normal logistic regression with an $L^1$-regularizer $\\implies$ \n",
    "                                its explanation is selective, as in sparse, which may be essential for \n",
    "                                providing a human-friendly explanation</li>\n",
    "                </ol>\n",
    "            </li>\n",
    "            <li><b>limitation:</b> <i>multicollinearity issue (input features are linearly correlated \n",
    "                with each other)</i>\n",
    "                <ul>\n",
    "                    <li>weights in a linear model are not necessarily intrinsically interpretable.\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b><a href=https://shap.readthedocs.io/en/latest/index.html>Kernel SHAP:</a></b>\n",
    "        <ul>\n",
    "            <li><i>Shapley values</i>\n",
    "                <ul>\n",
    "                    <li><b> ---------------------------------------------------> functionally-grounded and human-grounded </b></li>\n",
    "                    <li>compute Shapley values to avoid the multicollinearity issue: \n",
    "                        <i>fit a linear model for every permutation of features enabled. \n",
    "                    For example, if there are two features $\\{x_1, x_2\\}$, the shapley values \n",
    "                    would aggregate the weights from fitting the datasets with features \n",
    "                    $\\{\\varnothing\\}, \\{x_1\\}, \\{x_2\\}, \\{x_1, x_2\\}$. \n",
    "                    If there are $T$ features this would require $2^T$ models.</i>\n",
    "                    </li>\n",
    "                </ul>\n",
    "            <li><i>Kernel SHAP:</i> \n",
    "                producing Shapley values in a more tractable manner\n",
    "                <ul>\n",
    "                    <li>combines 3 ideas:\n",
    "                        <ol>\n",
    "                            <li>it reduces the number of features via a mapping function \n",
    "                                $h_\\mathbf{x}(\\mathbf{z})$,</li>\n",
    "                            <li>it uses <code>squared-loss</code> instead of <code>cross-entropy</code> \n",
    "                                by working on logits,</li>\n",
    "                            <li>it weights each observation by how many features there are \n",
    "                                enabled.\n",
    "                            </li>\n",
    "                            <li>\n",
    "                                \\begin{equation}\n",
    "                                \\begin{aligned}\n",
    "                                {\\mathbf{E}}_{SHAP}(\\mathbf{x}, c) = & argmin_{\\mathbf{w}} \n",
    "                                \\sum_{\\mathbf{z} \\in \\mathbb{Z}^M} \n",
    "                                \\pi(\\mathbf{z})\\ (f(h_\\mathbf{x}(\\mathbf{z}))_c - g(\\mathbf{z}))^2 \\\\\n",
    "                                &\\text{where } g(\\mathbf{z}) = \\mathbf{w} \\mathbf{z} \\\\\n",
    "                                &\\phantom{\\text{where }} \\pi(\\mathbf{z}) = \\frac{M - 1}\n",
    "                                {(M \\text{choose}\\, |\\mathbf{z}|) |\\mathbf{z}| (M - |\\mathbf{z}|)}\\\\\n",
    "                                \\end{aligned}\n",
    "                                \\label{eq:input-features:shap}\n",
    "                                \\end{equation}\n",
    "                                $\\mathbf{z}$ is a $\\{0,1\\}^M$ vector that describes which combined features \n",
    "                                are enabled.\n",
    "                            </li>\n",
    "                        </ol>\n",
    "                    </li>\n",
    "                    <li>popular for:\n",
    "                        <ol>\n",
    "                            <li>their mathematical foundation and the <code>shap</code> library</li>\n",
    "                            <li>the <code>shap</code> library also presents Partition SHAP which claims \n",
    "                                to reduce the number of model evaluations to $M^2$, instead of $2^M$</li>\n",
    "                        </ol>\n",
    "                    </li>\n",
    "                    <li>disadvantages:\n",
    "                        <ol>\n",
    "                            <li>it inherently depends on the masked inputs still being valid inputs. \n",
    "                                For some NLP models, this can be accomplished with a <code>MASK</code> token, \n",
    "                                while for it is not possible in a post-hoc setting.</li>\n",
    "                        </ol>\n",
    "                    </li>\n",
    "                    <li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    <li><b>Anchors: -------------------------------------------------------------------> human-groundnded</b>\n",
    "        <ul>\n",
    "            <li>Instead of giving an importance score, the Anchors simply provides a shortlist of \n",
    "                words that were most relevant for making the prediction\n",
    "            </li>\n",
    "            <li>human-groundness</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c292b9",
   "metadata": {},
   "source": [
    "<h1>Adversarial Examples</h1> \n",
    "<ul>\n",
    "    <li> ---------------------------------------------------------------------------> Human-grounded</li>\n",
    "    <li>Because the adversarial example serves as an explanation, in the context of an existing example it is a \n",
    "        $\\text{local explanation}$.\n",
    "    </li>\n",
    "    <li>Adversarial exmaples produced form an existing example support boundries of a given examples\n",
    "        therefore provides interpretabity.<b><i> this explanation can be similar to the input \n",
    "        feature methods</i></b>\n",
    "    </li>\n",
    "    <li>$\\text{adversarial examples}$ vs $\\text{imput features}$\n",
    "        <table>\n",
    "            <tr>\n",
    "                <th style=\"height:30px;width:50px;text-align: center;background-color: #96D4D4;\">\n",
    "                    $\\text{adversarial examples}$</th> \n",
    "                <th style=\"height:30px;width:50px;text-align: center;background-color: #96D4D4;\"> \n",
    "                    $\\text{imput features}$</th>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"height:50px;width:300px;text-align: left;\">adversarial explanations are \n",
    "                    $contrastive$, meaning they explain by comparing with another example</td>\n",
    "                <td style=\"height:50px;width:300px;text-align: left;\">input features explain only concerning \n",
    "                    the original example</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"height:50px;width:300px;text-align: left;\">more human-grounded\n",
    "                </td>\n",
    "                <td style=\"height:50px;width:300px;text-align: left;\">both human and functionaly-grounded\n",
    "                </td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </li>\n",
    "    <li><b>The goal</b> is to develop an adversarial method $A$, that maps from $\\mathbf{x}$ to \n",
    "        $\\tilde{\\mathbf{x}}$:\n",
    "        \\begin{equation}\n",
    "        A(\\mathbf{x}) \\rightarrow \\tilde{\\mathbf{x}}\n",
    "        \\end{equation}\n",
    "        <b>to ensure that an adverserial example method is functionally-grounded, \n",
    "            one only needs to assert that $argmax_i p(i|\\mathbf{x}) \\not= argmax_i p(i|\\tilde{\\mathbf{x}})$ \n",
    "            and that $\\mathbf{x}$ and $\\tilde{\\mathbf{x}}$ are paraphrases.\n",
    "        </b>\n",
    "    </li>\n",
    "</ul>\n",
    "<h3>HotFlip:</h3>\n",
    "<ul>\n",
    "    <li>relation between input feature explanations and adversarial examples</li>\n",
    "    <li><b>using gradients:</b> \n",
    "        the effect of changing token $v$ to another token $\\tilde{v}$ at position $t$, \n",
    "        on the model loss $\\mathcal{L}$, is estimated via using gradients\n",
    "        \\begin{equation}\n",
    "        \\mathcal{L}(y, \\tilde{\\mathbf{x}}_{t: v\\rightarrow \\tilde{v}}) - \n",
    "        \\mathcal{L}(y, \\mathbf{x}) \\approx \\frac{\\partial \\mathcal{L}(y, \\mathbf{x})}{\\partial x_{t,\\tilde{v}}} \n",
    "        - \\frac{\\partial \\mathcal{L}(y, \\mathbf{x})}{\\partial x_{t,v}},\n",
    "        \\end{equation}\n",
    "        where $\\tilde{\\mathbf{x}}_{t: v\\rightarrow \\tilde{v}}$ is the input $\\mathbf{x}$, \n",
    "        with the token $v$ position $t$ changed to $\\tilde{v}$.</li>\n",
    "    <li><b>without using gradient:</b> \n",
    "        exactly compute a forward pass for every possible token swap and requires one backward pass\n",
    "    </li>\n",
    "    <li><b>adversarial sentence with multiple tokens changed:</b> use a \n",
    "        <a href=https://towardsdatascience.com/an-intuitive-explanation-of-beam-search-9b1d744e7a0f>\n",
    "            beam-search</a> approach\n",
    "    </li>\n",
    "</ul>\n",
    "<h3>Semantically Equivalent Adversaries (SEA)</h3>\n",
    "<ul>\n",
    "    <li><b>adversarial examples have to be produced as paraphrases:</b> $\\implies$ we can sample examples \n",
    "        from a paraphrasing model $q(\\tilde{\\mathbf{x}} | \\mathbf{x})$.\n",
    "    </li>\n",
    "    <li>We need to <b>maximize the similarity</b>, while still having a <b>different model prediction.</b>\n",
    "    </li>\n",
    "    <li><b>how:</b> by measuring a semantical-equivalency-score $S(\\mathbf{x}, \\tilde{\\mathbf{x}})$, \n",
    "        as the relative likelihood of \n",
    "        $q(\\tilde{\\mathbf{x}} | \\mathbf{x})$ compared to $q(\\mathbf{x} | \\mathbf{x})$.\n",
    "    </li>\n",
    "    <li>\n",
    "        \\begin{equation}\n",
    "        \\begin{aligned}\n",
    "        A_{SEA}(\\mathbf{x}) = \n",
    "        argmax_{\\tilde{\\mathbf{x}} \\sim q(\\tilde{\\mathbf{x}} | \\mathbf{x})}\\ & S(\\mathbf{x}, \n",
    "        \\tilde{\\mathbf{x}}) \\\\\n",
    "        \\text{s.t. } S(\\mathbf{x}, \\tilde{\\mathbf{x}}) \\ge 0.8 \\\\\n",
    "        \\phantom{s.t. } argmax_i p(i|\\mathbf{x}) \\neq argmax_i p(i|\\tilde{\\mathbf{x}}) \\\\\n",
    "        \\text{where } S(\\mathbf{x}, \\tilde{\\mathbf{x}}) = \n",
    "        \\min\\left(1, \\frac{q(\\tilde{\\mathbf{x}} | \\mathbf{x})}{q(\\mathbf{x} | \\mathbf{x})}\\right)\n",
    "        \\end{aligned}\n",
    "        \\end{equation}\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540cdc86",
   "metadata": {},
   "source": [
    "<h1>Similar examples</h1>\n",
    "This explanation finds examples from the training dataset that looks like the input example from the model's perspective. ---------------------------------> local explanation\n",
    "\n",
    "- quite useful for discovering dataset artifacts\n",
    "- It is therefore critical that a similar examples explanation directly inform about how the model predicted the input example.\n",
    "\n",
    "<h2>Influence functions</h2>\n",
    "<ul>\n",
    "    <li>to estimate the effect on the loss  $\\mathcal{L}$, of removing the \n",
    "        observation $\\tilde{\\mathbf{x}}$ from the dataset.\n",
    "    </li>\n",
    "    <li><b>the loss difference:</b>\n",
    "        \\begin{equation}\n",
    "        \\mathcal{L}(y, \\mathbf{x}; \\tilde{\\theta}) - \\mathcal{L}(y, \\mathbf{x}; \\theta) \\approx \n",
    "        \\frac{1}{n} \\nabla_{\\theta} \\mathcal{L}(y, \\mathbf{x}; \\theta)^{\\top} H_{\\theta}^{-1} \n",
    "        \\nabla_{\\theta} \\mathcal{L}(\\tilde{y}, \\tilde{\\mathbf{x}}; \\theta).\n",
    "        \\label{eq:similar-examples:influence-functions:main}\n",
    "        \\end{equation}\n",
    "    </li>\n",
    "    <li><b>limitation:</b>\n",
    "        <ol>\n",
    "            <li>the explanation does not provide a direct indication of what exactly \n",
    "        about the similar examples that was important</li>\n",
    "            <li>computing the influence functions is not always numerically stable, because the equation\n",
    "        uses the gradient $\\nabla_{\\theta} \\mathcal{L}(\\tilde{y}, \\tilde{\\mathbf{x}}; \\theta)$ \n",
    "        which is optimized to be close to zero.\n",
    "            </li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><b>Performance considerations:</b> \n",
    "        $\\nabla_{\\theta} \\mathcal{L}(y, \\mathbf{x}; \\theta)^{\\top} H_{\\theta}^{-1}$ \n",
    "        can be cached for each test example but it is still too computationally intensive for \n",
    "        real-time inspection of the model $\\implies$ only use a subset of training data, using a \n",
    "        KNN clustering\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<h2>Representer Point Selection</h2>\n",
    "----------------------------------------------------------------------------------> human-grounded\n",
    "<ul>\n",
    "    <li>An alternative to influence functions is the <b>Representer theorem</b></li>\n",
    "    <li><b>idea:</b> the <a href=http://www.columbia.edu/~so33/SusDev/Lecture_9.pdf> logits</a> \n",
    "        of a test example $\\mathbf{x}$ can be expressed as a decomposition of \n",
    "        all training samples $f(\\mathbf{x}) = \\sum_{i=1}^n {\\mathbf{\\alpha}}_i \\kappa(\\mathbf{x},\n",
    "        \\tilde{\\mathbf{x}}_i)$.\n",
    "    </li>\n",
    "    <li> numerically stable than influence functions</li>\n",
    "    <li> only depending on intermediate representation of the final layer, \n",
    "        while influence functions employs the entire model\n",
    "    </li>\n",
    "</ul>\n",
    "<h2>Counterfactuals</h2>\n",
    "<ul>\n",
    "    <li>answering the question <b>how would the input need to change for the prediction to be different?</b></li>\n",
    "    <li>a minimal-edit from the original example and fluent</li>\n",
    "    <li><b>adversarial examples vs counterfactual examples</b>\n",
    "        <table>\n",
    "            <tr>\n",
    "                <th style=\"height:30px;width:50px;text-align: center;background-color: #96D4D4;\">\n",
    "                    $\\text{adversarial examples}$</th> \n",
    "                <th style=\"height:30px;width:50px;text-align: center;background-color: #96D4D4;\"> \n",
    "                    $\\text{counterfactual examples}$</th>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"height:50px;width:300px;text-align: center;\">paraphrases of the original example</td>\n",
    "                <td style=\"height:50px;width:300px;text-align: center;\">semantically opposite</td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </li>\n",
    "    <li><b>Polyjuice</b></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f8936b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cb2f87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
