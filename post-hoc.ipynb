{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "befc84c6",
   "metadata": {},
   "source": [
    "<h1> <a href=https://arxiv.org/abs/2108.04840><b>Post-hoc Interpretability for Neural NLP:</b> A Survey</a></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0297f16a",
   "metadata": {},
   "source": [
    "<h2>This survey:</h2>\n",
    "<ul>\n",
    "    <li>provides a categorization of how recent interpretability methods communicate explanations and discusses the methods in depth</li>\n",
    "    <li>focuses on post-hoc methods, which provide explanations after a model is learned and generally model-agnostic</li>\n",
    "    <li>discusses how these post-hoc methods are evaluated since the concern is whether these interpretabilities accurately reflect the model.</li>\n",
    "    <li>\n",
    "        <table>\n",
    "            <tr>\n",
    "                <th style=\"height:30px;width:50px;text-align: center;background-color: #96D4D4;\">$\\text{post-hoc}$</th> \n",
    "                <th style=\"height:30px;width:50px;text-align: center;background-color: #96D4D4;\"> $\\text{intrinsic}$</th>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"height:50px;width:300px;text-align: left;\">methods that provide their explanation after a model is trained and are often model-agnostic</td>\n",
    "                <td style=\"height:50px;width:300px;text-align: left;\">the model architecture itself helps to provide the explanation $\\implies$ interpretable by design</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"height:50px;width:300px;text-align: left;\">valuable in legal proceedings, where models may need to be explained retroactively,\n",
    "                </td>\n",
    "                <td style=\"height:50px;width:300px;text-align: left;\">more responsible to use in high-stakes decision processes,\n",
    "                </td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"height:50px;width:300px;text-align: left;\">it is guaranteed that they will not affect model performance.\n",
    "                </td>\n",
    "                <td style=\"height:50px;width:300px;text-align: left;\">\n",
    "                </td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </li>\n",
    "    <li>presents organizational interpretability methods by how and what they selectively communicate. For example, <i>input feature </i>explanations communicate what tokens are most relevant for a prediction\n",
    "    </li>\n",
    "    <li>Communication strategies' abstraction levels:\n",
    "        <table>\n",
    "            <tr>\n",
    "                <th style=\"height:30px;width:50px;text-align: center;background-color: #96D4D4;\">$\\text{less abstract}$</th>\n",
    "                <th style=\"height:30px;width:50px;text-align: center;background-color: #96D4D4;\">$\\text{more abstract}$</th>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"height:30px;width:300px;text-align: center;\">local explanations</td>\n",
    "                <td style=\"height:30px;width:300px;text-align: center;\">global explanations</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"height:100px;width:300px;text-align: left;\">\n",
    "                    <ul>\n",
    "                        <li>Input Features</li>\n",
    "                        <li></li>\n",
    "                        <li></li>\n",
    "                    </ul>\n",
    "                </td>\n",
    "                <td style=\"height:100px;width:300px;text-align: left;\">global explanations</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"height:50px;width:300px;text-align: left;\">e.g., <b>input features explanation</b>\n",
    "which highlights the input tokens that are most responsible for a prediction refer to <code>specific tokens</code>, therefore, its ability to provide abstract explanations is limited.</td>\n",
    "                <td style=\"height:50px;width:300px;text-align: left;\">e.g., <b>natural language</b> category which explains a prediction using <code>a sentence</code> and can therefore use abstract concepts in its explanation.</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"height:50px;width:300px;text-align: left;\">not easier to understand, but the trade-off is that they may reflect the model’s behavior <b>more</b></td>\n",
    "                <td style=\"height:50px;width:300px;text-align: left;\">easier to understand, but the trade-off is that they may reflect the model’s behavior <b>less</b></td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </li>\n",
    "</ul>\n",
    "</ul>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f1b17d",
   "metadata": {},
   "source": [
    "<h2>Accountability:</h2>\n",
    "<ul>\n",
    "    <li><b>definition:</b>\n",
    "        <ul>\n",
    "            <li> “the quality or state of being accountable, i.e., subject to giving an account and capable of being explained”. Accordingly, accountability is mainly related to auditing algorithms and verifying they perform as expected.</li>\n",
    "            <li>The ability to inspect a model in post hoc, and make it available for human or algorithmic inspection. </li>\n",
    "            <li>“Accountable” means “expected or required to account for one’s actions; answerable.” “Accountability” therefore means the ability, inclination or suitability to make someone answerable for his or her actions. According to the OECD, accountability means the ability to place the onus on the\n",
    "appropriate organizations or individuals for the proper functioning of AI systems</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "<h2>Interpretability:</h2>\n",
    "<ol>\n",
    "    <li><b>the goal:</b> is to communicate the model to a human (often qualitatively)</li>\n",
    "    <li>Due to their immense complexity motivated by general correlation between their size and test performance, <b>balck-box</b> models exhibit unwanted biases and similar ethical issues,</li>\n",
    "    <li>these issues stem from an <i style=\"background-color:orange;\">``incompleteness in the problem formalization``.</i>, meaning if the model was contrained and optimized to prevent all possible ethical issues, interpretability would be much less relevant</li>\n",
    "    <li> these issues can be partially prevented (but not for all failure modes) with $robustness$ and $fairness$ metrics</li>\n",
    "    <li><b>The goal of interpretability</b> is therefore providing <i style=\"background-color:lime;\">$\\text{quality assessment}$</i> through <i style=\"background-color:lime;\">$\\text{model explanations}$</i> to to facilitate the <i style=\"background-color:lime;\">$\\text{accountability process}$</i></li>\n",
    "    <li>definition (<a href=https://arxiv.org/abs/1702.08608>Doshi-Velez and Kim (2017)</a>): <i>the ability to explain or to present in <b style=\"background-color:yellow;\">understandable</b> terms to a human</i></li>\n",
    "    <li>We consider <b style=\"background-color:yellow;\">understandable</b> as an interdisciplinary concept</li>\n",
    "    <li>definition (<a href=https://arxiv.org/abs/1706.07269>Miller (2019)</a>): effective explanations must be selective in the sense one must select “one or two causes from a sometimes infinite number of causes”.  Such observation necessitates organizing interpretability methods by <i style=\"background-color:red;\">how</i> and <i style=\"background-color:red;\">what</i> they selectively <b>communicate</b>.</li>\n",
    "</ol>\n",
    "<h2>Communication:</h2>\n",
    "        <ul>\n",
    "            <li>communication methods in higher abstraction level are easier to understrand but they may reflect the model’s behavior less</li>\n",
    "            <li>a trade-off between <b>abstraction</b> and <b>model behaviour</b> is necessary</li>\n",
    "        </ul>\n",
    "<h2>Motivation for interpretability:</h2>\n",
    "<ol type=\"A\">\n",
    "    <li><b>incompleteness in the problem formalization</b></li>\n",
    "    <li><b>safety</b>\n",
    "        ensuring the model performs within expectations in deployment\n",
    "        <ul>\n",
    "            <li> it is nearly impossible to truly test the model,</li>\n",
    "            <li>adversarial examples and counterfactuals are useful, as they evaluate the model on data outside the test distribution.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>ethics</b>\n",
    "        ensuring that the model’s behavior is aligned with common ethical and moral values\n",
    "        <ul>\n",
    "            <li>should be judged qualitatively by humans,</li>\n",
    "            <li>to some extend involve qualitative assessment,</li>\n",
    "            <li>in some cases it may be possible to measure and satisfy this ethical concern via fairness metrics and debiasing techniques</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>accountability</b>\n",
    "    relates to explaining the model when it does fail in production\n",
    "    </li>\n",
    "    <li><b>scientific understanding</b>\n",
    "    addresses a need by researchers and scientists, which is to enerate hypotheses and knowledge\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126ec7ef",
   "metadata": {},
   "source": [
    "<h2>Measuring the interpretability:</h2>\n",
    "<ol type=\"i\">\n",
    "    <li><b>Application-grounded</b>\n",
    "        <ol>\n",
    "            <li>the interpretability method is evaluated in the environment it will be deployed</li>\n",
    "            <li>should include the baseline where the explanations are provided by humans</li>\n",
    "            <li>application-specific and long-term nature of this approach $\\implies$  rarely done in NLP</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><b>Human-grounded:</b>\n",
    "        <ol>\n",
    "            <li>also known as <b>plausibility</b>, <b>simulatability</b>, and\n",
    "<b>comprehensibility</b></li>\n",
    "            <li>checks if the explanations are useful to lay humans instead of domain experts</li>\n",
    "            <li>the task is often simpler and can be evaluated immediately</li>\n",
    "            <li>often involves interdisciplinary knowledge from the human-computer interaction (HCI) and social science fields</li>\n",
    "            <li>can not reflect the model perfectly, because humans require explanations to be selective, meaning the explanation should select “one or two causes from a sometimes infinite number of causes</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><b>Functionally-grounded:</b>\n",
    "        <ol>\n",
    "            <li>also known as <b>faithfulness</b> and have also been referred to as <b>fidelity</b></li>\n",
    "            <li>checks how well the explanation reflects the model,</li>\n",
    "            <li>checks if the explanations are useful to lay humans instead of domain experts</li>\n",
    "            <li>the task is often simpler and can be evaluated immediately</li>\n",
    "            <li>often involves interdisciplinary knowledge from the human-computer interaction (HCI) and social science fields</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><b>common evaluation strategies</b>\n",
    "        <ul>\n",
    "            <li>Comparing with an intrinsically interpretable model, such as <code>logistic regression</code></li>\n",
    "            <li>Comparing with other post-hoc methods</li>\n",
    "            <li>Proposing axiomatic desirables</li>\n",
    "            <li>Benchmarking against random explanations</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daca61a7",
   "metadata": {},
   "source": [
    "<h1>Interpretability methods</h1>\n",
    "<ol>\n",
    "    <li><b>Input Features:</b>\n",
    "                determine how important an input feature \n",
    "                (<i>always known and often meaningful to humans</i>), e.g. a token, is for a given prediction. \n",
    "                input $\\mathbf{x}$,\n",
    "                \\begin{equation}\n",
    "                \\mathbf{E}(\\mathbf{x}, c): \n",
    "                \\mathrm{I}^\\mathbf{d} \\rightarrow \\mathbb{R}^\\mathbf{d} \\text{, where$\\mathrm{I}$ \n",
    "                is the input domain and $\\mathbf{d}$ is the input dimensionality.}\n",
    "                \\end{equation}\n",
    "        <ul>\n",
    "            <li><i>when the output is a score of importance the explanation is called \n",
    "                    an importance measure\n",
    "                </i></li>\n",
    "            <li>can only explain one class at a time</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>Gradient:</b>\n",
    "                \\begin{equation}\n",
    "                \\mathbf{E}_{\\operatorname{gradient}}(\\mathbf{x}, c) = \n",
    "                \\nabla_\\mathbf{x} f(\\mathbf{x})_c, \\text{where $f(\\mathbf{x})$ is the model logits.}\n",
    "                \\end{equation}\n",
    "        <ul>\n",
    "            <li><a href=https://arxiv.org/pdf/1810.03292.pdf> for a linear model</a> $f(\\mathbf{x}) = \\mathbf{x}\\mathbf{W}$, the explanation would be $\\mathbf{W}^\\top$ \n",
    "                which is clearly a valid explanation\n",
    "            </li>\n",
    "            <li>this does not guarantee functionally-groundedness \n",
    "                <a href=https://aclanthology.org/N16-1082> for non-linear models</a> \n",
    "                we approximate $f(\\mathbf{x})$ with a linear function by computing the \n",
    "                first-order Taylor expansion\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>Integrated Gradient (IG)</b>:\n",
    "        <ul>\n",
    "            <li>$Sensitivity$ means, if there exists a combination of $\\mathbf{x}$ and baseline \n",
    "            $\\mathbf{b}$ (often an empty sequence), where the output of $f(\\mathbf{x})$ and \n",
    "            $f(\\mathbf{b})$ are different, then the feature that changed should get a non-zero attribution. \n",
    "            This desirable is not satisfied for the gradient method, \n",
    "            for example due to the truncation  in $ReLU(\\cdot)$.</li>\n",
    "            <li>$Completeness$ means, the sum of importance scores assigned to each token should \n",
    "                equal the model output relative to the baseline $\\mathbf{b}$.\n",
    "                \\begin{equation}\n",
    "                \\mathbf{E}_{\\operatorname{integrated-gradient}}(\\mathbf{x}, c) = \n",
    "                (\\mathbf{x} - \\mathbf{b}) \\odot \\frac{1}{k} \\sum_{i=1}^{k} \n",
    "                \\nabla_{\\tilde{\\mathbf{x}}_i} f(\\tilde{\\mathbf{x}}_i)_c, \\quad \\tilde{\\mathbf{x}}_i = \n",
    "                \\mathbf{b} + \\frac{i}{k}(\\mathbf{x} - \\mathbf{b}) \n",
    "                \\label{eq:input-features:integrated-gradient:formulation}\n",
    "                \\end{equation}\n",
    "            </li>\n",
    "            <li>has recently received criticism in computer vision (CV) community for \n",
    "                not being functionally-grounded. One reason is that it multiples by the input, a signal \n",
    "                that is not directly related to the model</li>\n",
    "        </ul>\n",
    "    <li><b>LIME:</b>\n",
    "        <ul>\n",
    "            <li>Instead of relying on gradient, it samples nearby observations $\\tilde{\\mathbf{x}}$ and \n",
    "                uses the model estimate $p(c|\\tilde{\\mathbf{x}})$ to fit a logistic regression. \n",
    "                The parameters $\\mathbf{w}$ of the logistic regression then represents the \n",
    "                importance measure, since larger parameters would mean a \n",
    "                greater effect on the output.\n",
    "                \\begin{equation}\n",
    "                \\begin{aligned}\n",
    "                \\mathbf{E}_{\\operatorname{LIME}}(\\mathbf{x}, c) = & argmin_{\\mathbf{w}} \n",
    "                \\frac{1}{k} \\sum_{i=1}^k \\left(p(c|\\tilde{\\mathbf{x}}_i) \\log(q(\\tilde{\\mathbf{x}}_i)) + \n",
    "                (1-p(c|\\tilde{\\mathbf{x}}_i)) \\log(1-q(\\tilde{\\mathbf{x}}_i)\\right) + \n",
    "                \\lambda \\|\\mathbf{w}\\|_1 \\\\ &\\text{ where } q(\\tilde{\\mathbf{x}}) = \n",
    "                \\sigma(\\mathbf{w} \\tilde{\\mathbf{x}})\n",
    "                \\end{aligned}\n",
    "                \\end{equation}\n",
    "            </li>\n",
    "            <li>how to sample $\\tilde{\\mathbf{x}}$ representing the nearby observations:\n",
    "                <ol>\n",
    "                    <li>use a Bag-Of-Words (BoW) representation with a cosine distance \n",
    "                        (distance metrics may not effectively match the model's internal space)</li>\n",
    "                    <li>sample $\\tilde{\\mathbf{x}}$ by masking words of $\\mathbf{x}$ \n",
    "                        (requires a model that supports such masking)</li>\n",
    "                </ol>\n",
    "            </li>\n",
    "            <li><b>advantages:</b> \n",
    "                <ol>\n",
    "                    <li>only depends on black-box information and the dataset, therefore \n",
    "                        no gradient calculations are required</li>\n",
    "                    <li>it uses a <code>LASSO logistic regression</code>, which is a \n",
    "                                normal logistic regression with an $L^1$-regularizer $\\implies$ \n",
    "                                its explanation is selective, as in sparse, which may be essential for \n",
    "                                providing a human-friendly explanation</li>\n",
    "                </ol>\n",
    "            </li>\n",
    "            <li><b>limitation:</b> <i>multicollinearity issue (input features are linearly correlated \n",
    "                with each other)</i>\n",
    "                <ul>\n",
    "                    <li>weights in a linear model are not necessarily intrinsically interpretable.\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>Kernel SHAP:</b>\n",
    "        <ul>\n",
    "            <li>compute Shapley values to avoid the multicollinearity issue: \n",
    "                <i>fit a linear model for every permutation of features enabled. \n",
    "                    For example, if there are two features $\\{x_1, x_2\\}$, the shapley values \n",
    "                    would aggregate the weights from fitting the datasets with features \n",
    "                    $\\{\\varnothing\\}, \\{x_1\\}, \\{x_2\\}, \\{x_1, x_2\\}$. \n",
    "                    If there are $T$ features this would require $2^T$ models.</i>\n",
    "            </li>\n",
    "            <li>combines 3 ideas:\n",
    "                <ol>\n",
    "                    <li>it reduces the number of features via a mapping function \n",
    "                                $h_\\mathbf{x}(\\mathbf{z})$,</li>\n",
    "                    <li>it uses <code>squared-loss</code> instead of <code>cross-entropy</code> \n",
    "                                by working on logits,</li>\n",
    "                    <li>it weights each observation by how many features there are \n",
    "                                enabled.\n",
    "                    </li>\n",
    "                    <li>\n",
    "                        \\begin{equation}\n",
    "                        \\begin{aligned}\n",
    "                        \\mathbf{E}_{\\operatorname{SHAP}}(\\mathbf{x}, c) = & argmin_{\\mathbf{w}} \n",
    "                        \\sum_{\\mathbf{z} \\in \\mathbb{Z}^M} \n",
    "                        \\pi(\\mathbf{z})\\ (f(h_\\mathbf{x}(\\mathbf{z}))_c - g(\\mathbf{z}))^2 \\\\\n",
    "                        &\\text{where } g(\\mathbf{z}) = \\mathbf{w} \\mathbf{z} \\\\\n",
    "                        &\\phantom{\\text{where }} \\pi(\\mathbf{z}) = \\frac{M - 1}\n",
    "                        {(M \\text{choose}\\, |\\mathbf{z}|) |\\mathbf{z}| (M - |\\mathbf{z}|)}\\\\\n",
    "                        \\end{aligned}\n",
    "                        \\label{eq:input-features:shap}\n",
    "                        \\end{equation}\n",
    "                        $\\mathbf{z}$ is a $\\{0,1\\}^M$ vector that describes which combined features are enabled.\n",
    "                    </li>\n",
    "                </ol>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b></b></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061a0b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
