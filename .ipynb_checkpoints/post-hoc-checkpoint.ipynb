{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "befc84c6",
   "metadata": {},
   "source": [
    "<h1> <a href=https://arxiv.org/abs/2108.04840><b>Post-hoc Interpretability for Neural NLP:</b> A Survey</a></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0297f16a",
   "metadata": {},
   "source": [
    "<h2>This survey:</h2>\n",
    "<ul>\n",
    "    <li>provides a categorization of how recent interpretability methods communicate explanations and discusses the methods in depth</li>\n",
    "    <li>focuses on post-hoc methods, which provide explanations after a model is learned and generally model-agnostic</li>\n",
    "    <li>discusses how these post-hoc methods are evaluated since the concern is whether these interpretabilities accurately reflect the model.</li>\n",
    "    <li>presents organizational interpretability methods by how and what they selectively communicate. For example, <i>input feature </i>explanations communicate what tokens are most relevant for a prediction</li>\n",
    "    <li>\n",
    "        <table>\n",
    "            <tr>\n",
    "                <th style=\"height:30px;width:50px;text-align: center;\">$\\text{post-hoc}$</th> \n",
    "                <th style=\"height:30px;width:50px;text-align: center;\">$\\text{intrinsic}$</th>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <th style=\"height:50px;width:300px;text-align: left;\">methods that provide their explanation after a model is trained and are often model-agnostic</th>\n",
    "                <th style=\"height:50px;width:300px;text-align: left;\">the model architecture itself helps to provide the explanation</th>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<h2>Motivation:</h2>\n",
    "<ul>\n",
    "    <li>Interpretability:\n",
    "        <ol>\n",
    "            <li><b>the goal:</b> is to communicate the model to a human</li>\n",
    "            <li>Due to their immense complexity motivated by general correlation between their size and test performance, <b>balck-box</b> models exhibit unwanted biases and similar ethical issues,</li>\n",
    "            <li>these issues stem from an <i style=\"background-color:orange;\">``incompleteness in the problem formalization``</i></li>\n",
    "            <li> these issues can be partially prevented (but not for all failure modes) with $robustness$ and $fairness$ metrics</li>\n",
    "            <li><b>The goal of interpretability</b> is therefore providing <i style=\"background-color:lime;\">$\\text{quality assessment}$</i> through <i style=\"background-color:lime;\">$\\text{model explanations}$</i> to to facilitate the <i style=\"background-color:lime;\">$\\text{accountability process}$</i></li>\n",
    "            <li>definition (<a href=https://arxiv.org/abs/1702.08608>Doshi-Velez and Kim (2017)</a>): <i>the ability to explain or to present in <b style=\"background-color:yellow;\">understandable</b> terms to a human</i></li>\n",
    "            <li>We consider <b style=\"background-color:yellow;\">understandable</b> as an interdisciplinary concept</li>\n",
    "            <li>definition (<a href=https://arxiv.org/abs/1706.07269>Miller (2019)</a>): effective explanations must be selective in the sense one must select “one or two causes from a sometimes infinite number of causes”.  Such observation necessitates organizing interpretability methods by <i style=\"background-color:red;\">how</i> and <i style=\"background-color:red;\">what</i> they selectively <b>communicate</b>.</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>Communication:\n",
    "        <ul>\n",
    "            <li>communication methods in higher abstraction level are easier to understrand but they may reflect the model’s behavior less</li>\n",
    "            <li>a trade-off between <b>abstraction</b> and <b>\\text{model behaviour}</b> is necessary</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e064eed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a37542",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
