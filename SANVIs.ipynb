{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SANVIs.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOq8EQ/vkrTqI0f8gYshoYw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Masoud-Karami/Abstracts/blob/main/SANVIs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5Fjq2GVaIpk"
      },
      "source": [
        "## **[SANVis](https://arxiv.org/abs/1909.09595)**: Visual Analytics for Understanding Self-Attention Networks [code](http://short.sanvis.org/)\n",
        "\n",
        "> **multi-head self-attention networks** : \n",
        ">>1. A set of input vectors, e.g., word vectors $\\rightarrow$ into another set of vectors\n",
        ">>2. Simultaneously capturing syntactic and semantic features, each of which corresponds to a particular attention head\n",
        ">>3. Model complexity $\\implies$ difficult to understand\n",
        ">>4. two currently used pre-training methods\n",
        "    <ol type=\"i\">\n",
        "        <li> autoregressive language modeling</li>\n",
        "        <li> autoencoding </li> (e.g. BERT)\n",
        "    </ol>\n",
        ">>5. The proble of BERT in not being pre-trained autoregressively is becomeing clear in the example in the [XLNet](https://arxiv.org/abs/1906.08237) paper. \n",
        ">>> $ \\text{Considering} \\log p(\\text{New York} \\mid \\text{is a city})\n",
        "\\mathcal{J}_{\\text{BERT}} = \\log p(\\text{New} \\mid \\text{is a city}) + \\log p(\\text{York} \\mid \\text{is a city}),$ inwhich each of the probabilities are indipendent from each other. It ma couse the following problem but a pefect match in BERT's mind: \n",
        "$**\\text{Los York is a city}**$.\n",
        "However with XLNet:\n",
        "$\\mathcal{J}_{\\text{XLNet}} = \\log p(\\text{New} \\mid \\text{is a city}) + \\log p(\\text{York} \\mid {\\text{New}}, \\text{is a city}).$\n",
        ">>6. SANVis is **interactive** visual analysis in contrast to other non-interactive research such as [1](https://arxiv.org/abs/1804.08199)and [2](https://arxiv.org/abs/1905.09418?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%253A+arxiv%252FQSXk+%2528ExcitingAds%2521+cs+updates+on+arXiv.org%2529) which rely on removing unnecessary heads to improve the prediction accuracy.\n",
        ">>7. Each layer in the **encoder** includes two sequential sub-layers a *multi-head self-attention*, and a *position-wise feed-forward network*.\n",
        ">>8. The **decoder** has an additional attention layer, which is called as an *encoder-decoder attention* to give the model an insight to the encoder's internal\n",
        ">>9. Both encoder and decoder layers consist of **skip-connection** and **layer normalization**. \n",
        ">>10. Transformation metrics of each head in multi-head attention are randomly initialized $\\implies$ different representation $\\implies$ different attention shapes and patterns $\\implies$ each head attention differently to the adjacent words or linguistics relations. \n",
        ">>11.\n",
        ">>\n",
        ">>layer          | input \n",
        ">>---------------|:-----------\n",
        ">>`*encoder*` |source words as the input to the query, key, and value transformations\n",
        ">>`*decoder*`        |target words of the decoder\n",
        ">>`*encoder-decoder*`|target words as input to a query transformation but source words as the input to a key and a value transformation\n",
        ">>\n",
        ">>12. attention pattterns:\n",
        ">>>   <ul type='square'>\n",
        "        <li> diagonal patterns indicating that a query words attend s to itself</li> \n",
        ">>>        \n",
        ">>>![picture](https://drive.google.com/uc?export=view&id=1RslqE-Oao64iTFmddXWGA30V0vZw76vV)\n",
        ">>><li>the pattern that indicates a query attends to its immediate previous word</li>\n",
        ">>>\n",
        ">>>![picture](https://drive.google.com/uc?export=view&id=1mlP1vaik9tOt3ybSPAqRvkYIx9CIQozN)\n",
        ">>><li>the pattern that indicates a query attends to its immediate next word</li>\n",
        ">>>\n",
        ">>>![picture](https://drive.google.com/uc?export=view&id=1IqGOf8661bFUzn1ZeAwDRU5fYlspn8YN)\n",
        ">>><li>the pattern that indicates a query attends to a common single word</li>\n",
        ">>>\n",
        ">>>![picture](https://drive.google.com/uc?export=view&id=1K8UlyrhEoNamL9Z1BZHR02BLTgkXQwGu)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBy7gUmvSH-g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}