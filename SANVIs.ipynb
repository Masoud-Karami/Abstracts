{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SANVIs.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNJ8/vMS5NW9x1XdT/QUYw/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Masoud-Karami/Abstracts/blob/main/SANVIs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfqRDogg05li"
      },
      "source": [
        "# **[SANVis](https://arxiv.org/abs/1909.09595)**: Visual Analytics for Understanding Self-Attention Networks [code](http://short.sanvis.org/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5Fjq2GVaIpk"
      },
      "source": [
        "\n",
        "\n",
        "##**multi-head self-attention networks** : \n",
        ">1. A set of input vectors, e.g., word vectors $\\rightarrow$ into another set of vectors\n",
        ">2. Simultaneously capturing syntactic and semantic features, each of which corresponds to a particular attention head\n",
        ">3. Model complexity $\\implies$ difficult to understand\n",
        ">4. two currently used pre-training methods\n",
        "    <ol type=\"i\">\n",
        "        <li> <code><a href=\"https://www.cs.toronto.edu/~rgrosse/courses/csc421_2019/readings/L15%20Autoregressive%20and%20Reversible%20Models.pdf\">autoregressive</a></code> or Masked Language Modeling (<code><a href=\"https://arxiv.org/abs/2107.13955\">MLM</a></code>)</li>\n",
        "given a text sequence $\\mathbf{x} = (x_1, \\cdots, x_T)$, AR language modeling factorizes the likelihood into a forward product $p(\\mathbf{x}) = \\prod_{t=1}^{T} p(x_t \\mid \\mathbf{x}_{<t})$ or a backward one $p(\\mathbf{x}) = \\prod_{t=T}^{1} p(x_t \\mid \\mathbf{x}_{>t})$.\n",
        "        <li> <code>autoencoding</code> </li> AE based pretraining does not perform explicit density estimation but instead aims to reconstruct the original data from corrupted input. (e.g. BERT) $p(\\mathbf{x}) =^{??} \\prod_{t=1}^{T} p(x_t \\mid \\mathbf{x}_{\\neq t})$\n",
        "    </ol>\n",
        ">5. The problem of BERT in not being pre-trained autoregressively is becomeing clear in the example in the [XLNet](https://arxiv.org/abs/1906.08237) paper. \n",
        ">> $ \\text{Considering} \\log p(\\text{New York} \\mid \\text{is a city})\n",
        "\\mathcal{J}_{\\text{BERT}} = \\log p(\\text{New} \\mid \\text{is a city}) + \\log p(\\text{York} \\mid \\text{is a city}),$ inwhich each of the probabilities are indipendent from each other. It ma couse the following problem but a pefect match in BERT's mind: \n",
        "$**\\text{Los York is a city}**$.\n",
        "However with XLNet:\n",
        "$\\mathcal{J}_{\\text{XLNet}} = \\log p(\\text{New} \\mid \\text{is a city}) + \\log p(\\text{York} \\mid {\\text{New}}, \\text{is a city}).$\n",
        ">6. SANVis is **interactive** visual analysis in contrast to other non-interactive research such as [1](https://arxiv.org/abs/1804.08199)and [2](https://arxiv.org/abs/1905.09418?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%253A+arxiv%252FQSXk+%2528ExcitingAds%2521+cs+updates+on+arXiv.org%2529) which rely on removing unnecessary heads to improve the prediction accuracy.\n",
        ">7. Each layer in the **encoder** includes two sequential sub-layers a *multi-head self-attention*, and a *position-wise feed-forward network*.\n",
        ">8. The **decoder** has an additional attention layer, which is called as an *encoder-decoder attention* to give the model an insight to the encoder's internal\n",
        ">9. Both encoder and decoder layers consist of **skip-connection** and **layer normalization**. \n",
        ">10. Transformation metrics of each head in multi-head attention are randomly initialized $\\implies$ different representation $\\implies$ different attention shapes and patterns $\\implies$ each head attention differently to the adjacent words or linguistics relations. \n",
        ">11.\n",
        ">\n",
        ">>layer          | input \n",
        ">>---------------|:-----------\n",
        ">>`*encoder*` |source words as the input to the query, key, and value transformations\n",
        ">`*decoder*`        |target words of the decoder\n",
        ">`*encoder-decoder*`|target words as input to a query transformation but source words as the input to a key and a value transformation\n",
        ">\n",
        ">12. attention pattterns: ([images link](https://arxiv.org/abs/1909.09595))\n",
        ">>   <ul type='square'>\n",
        "        <li> diagonal patterns indicating that a query words attend s to itself</li> \n",
        ">>        \n",
        ">>![picture](https://drive.google.com/uc?export=view&id=1RslqE-Oao64iTFmddXWGA30V0vZw76vV)\n",
        ">><li>the pattern that indicates a query attends to its immediate previous word</li>\n",
        ">>\n",
        ">>![picture](https://drive.google.com/uc?export=view&id=1mlP1vaik9tOt3ybSPAqRvkYIx9CIQozN)\n",
        ">><li>the pattern that indicates a query attends to its immediate next word</li>\n",
        ">>\n",
        ">>![picture](https://drive.google.com/uc?export=view&id=1IqGOf8661bFUzn1ZeAwDRU5fYlspn8YN)\n",
        ">><li>the pattern that indicates a query attends to a common single word</li>\n",
        ">>\n",
        ">>![picture](https://drive.google.com/uc?export=view&id=1K8UlyrhEoNamL9Z1BZHR02BLTgkXQwGu)\n",
        ">></ul>\n",
        ">13. **feature vector** is the flattened $n^2$-dimensional vector of its ${A_{i}} \\in R^{T\\times T}$ attention matrix, where ${A}_{i}$ is calculated from $\\textrm{Softmax}\\left(\\frac{QK^{T}}{\\sqrt{d_{model}}}\\right)$ on the $i$-th head, concatenated with additional three-dimensional vector of (1) the sum of the upper triangular part of the matrix, (2) that of the lower-triangular part, and (3) the sum of diagonal entries.\n",
        ">14. \n",
        "    <ul>\n",
        "        <li>attention sorting</li>\n",
        "            <ul type='square'>\n",
        "                <li>entropy</li>\n",
        "                <li>position</li>\n",
        "            </ul>\n",
        "        <li>attention piling: computing and clustring feature vector of each attention head </li>\n",
        "    </ul>\n",
        ">15. $\\textbf{HeadLense:}$\n",
        ">>  <ol>\n",
        "        <li>$first:$ considering all the sentences to be included in\n",
        "a validation set and applying a query and a key transformation of input words for a given attention head and perform a `k-median` </li>\n",
        "        <li>$second$: obtain a <a href=\"https://stanford.edu/~cpiech/cs221/handouts/kmeans.html\">clusrer centroid</a> for key and query vectors</li>\n",
        "        <li>$third$: compute and visualize all the pairwise inner product similarities between each pair of a query cluster centroid and a key cluster one in a heatmap. High similarities means that the words\n",
        "in the query set are likely to attend to the words in the key set.</li>\n",
        "    </ol>\n",
        "              \n",
        "\n",
        "\n"
      ]
    }
  ]
}